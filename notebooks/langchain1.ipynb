{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates \n",
    "- Help to translate user input and parameters into instructions for a language model.\n",
    "- Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_msg = \"Translate the language from English to {language}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",system_msg), (\"user\",\"{text}\")]\n",
    ")\n",
    "prompt = prompt.invoke({\"language\":\"Italian\",\"text\":\"hi\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to access messages directly\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama2\")\n",
    "response = model.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents \n",
    "####  Documents are intended to represent a unit of text and associated metadata. It has three attributes:. It has three attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata;\n",
    "- id: (optional) a string identifier for the document.\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. \n",
    "### Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\n",
    "- When working with large datasets, you can use the .lazy_load method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in loader.lazy_load():\n",
    "#     print(document)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file = \"../attention is all you need.pdf\"\n",
    "loader = PyPDFLoader(file_path=file)\n",
    "docs = loader.load()\n",
    "print(docs[10].page_content[:200])\n",
    "print(\"Length:\",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "- For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation.\n",
    "- Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "- Large documents can dilute important information. Splitting helps improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,add_start_index=True)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\"\n",
    "https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "vectors = embeddings.embed_documents(split_docs[0].page_content)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector stores\n",
    "LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n",
    "https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our vectors in the vector store we can Perform things like similarity search etc.\n",
    "res = vector_store.similarity_search(\n",
    "    \"What is attention \"\n",
    ")\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vector_store.similarity_search_with_score(\"WHat is The goal of reducing sequential computation\")\n",
    "doc,score = res[0]\n",
    "print(\"Score\",score)\n",
    "print(\"Document\",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
