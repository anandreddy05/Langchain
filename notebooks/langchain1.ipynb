{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates \n",
    "- Help to translate user input and parameters into instructions for a language model.\n",
    "- Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the language from English to Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_msg = \"Translate the language from English to {language}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",system_msg), (\"user\",\"{text}\")]\n",
    ")\n",
    "prompt = prompt.invoke({\"language\":\"Italian\",\"text\":\"hi\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the language from English to Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to access messages directly\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao! (Chow)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama2\")\n",
    "response = model.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents \n",
    "####  Documents are intended to represent a unit of text and associated metadata. It has three attributes:. It has three attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata;\n",
    "- id: (optional) a string identifier for the document.\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. \n",
    "### Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\n",
    "- When working with large datasets, you can use the .lazy_load method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translati\n",
      "Length: 15\n"
     ]
    }
   ],
   "source": [
    "# for document in loader.lazy_load():\n",
    "#     print(document)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file = \"../attention is all you need.pdf\"\n",
    "loader = PyPDFLoader(file_path=file)\n",
    "docs = loader.load()\n",
    "print(docs[10].page_content[:200])\n",
    "print(\"Length:\",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "- For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation.\n",
    "- Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "- Large documents can dilute important information. Splitting helps improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,add_start_index=True)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\"\n",
    "https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.017090062,\n",
       "  0.0027207343,\n",
       "  0.0023896203,\n",
       "  -0.0036132417,\n",
       "  -0.018104615,\n",
       "  0.009382318,\n",
       "  0.023345228,\n",
       "  -0.011329206,\n",
       "  -0.016836721,\n",
       "  0.011749435,\n",
       "  0.0043983813,\n",
       "  -0.0009865034,\n",
       "  -0.022814523,\n",
       "  0.006527441,\n",
       "  0.002029811,\n",
       "  -0.0031695568,\n",
       "  3.539518e-05,\n",
       "  -0.013560653,\n",
       "  0.004701068,\n",
       "  -0.022765381,\n",
       "  -0.0020020534,\n",
       "  -0.027110893,\n",
       "  -0.00269133,\n",
       "  0.022735322,\n",
       "  0.0016181265,\n",
       "  -0.015245271,\n",
       "  -0.015044807,\n",
       "  0.011017536,\n",
       "  0.0061868504,\n",
       "  0.019850343,\n",
       "  0.005722294,\n",
       "  -0.019930165,\n",
       "  0.020516396,\n",
       "  0.009722108,\n",
       "  0.00087768614,\n",
       "  -0.03358023,\n",
       "  -0.033772964,\n",
       "  0.008089579,\n",
       "  -0.0039116605,\n",
       "  0.0065230937,\n",
       "  0.018194132,\n",
       "  0.005937826,\n",
       "  0.024696043,\n",
       "  -0.024083162,\n",
       "  -0.014224839,\n",
       "  0.003364533,\n",
       "  0.008830009,\n",
       "  0.0071857334,\n",
       "  -0.0098787565,\n",
       "  0.013772205,\n",
       "  -0.025089337,\n",
       "  -0.0025376284,\n",
       "  -0.0024273372,\n",
       "  -0.008117998,\n",
       "  0.011824772,\n",
       "  -0.006275674,\n",
       "  0.009378758,\n",
       "  -0.005720378,\n",
       "  0.008937054,\n",
       "  0.0130867,\n",
       "  -0.0010207263,\n",
       "  -0.02332093,\n",
       "  -0.017634887,\n",
       "  -0.019111214,\n",
       "  0.00964566,\n",
       "  0.01814479,\n",
       "  -0.014108669,\n",
       "  0.0063223224,\n",
       "  0.021152394,\n",
       "  -0.024768963,\n",
       "  -0.0037681567,\n",
       "  -0.014074003,\n",
       "  0.0035170966,\n",
       "  -0.013821024,\n",
       "  0.02844219,\n",
       "  0.03506883,\n",
       "  0.013821389,\n",
       "  -0.009935959,\n",
       "  0.01756148,\n",
       "  -0.015928945,\n",
       "  -0.0039722365,\n",
       "  -0.028110044,\n",
       "  -0.02001906,\n",
       "  -0.0058372174,\n",
       "  0.0037274451,\n",
       "  -0.014832051,\n",
       "  0.01361139,\n",
       "  -0.00914451,\n",
       "  0.004051022,\n",
       "  0.006881814,\n",
       "  -0.012676497,\n",
       "  -0.00089778216,\n",
       "  0.02591355,\n",
       "  0.016094526,\n",
       "  -0.0024165602,\n",
       "  0.010996069,\n",
       "  0.0028885999,\n",
       "  0.00034888068,\n",
       "  -0.0018144657,\n",
       "  0.0234742,\n",
       "  0.003092212,\n",
       "  0.035303444,\n",
       "  0.016852766,\n",
       "  0.015043137,\n",
       "  -0.0016050092,\n",
       "  -0.008676767,\n",
       "  -0.0064945742,\n",
       "  -0.0013762539,\n",
       "  -0.008255658,\n",
       "  0.0068542915,\n",
       "  -0.0005548814,\n",
       "  0.015285223,\n",
       "  -0.026045866,\n",
       "  -0.0019648846,\n",
       "  0.005326087,\n",
       "  0.0031696751,\n",
       "  0.004893514,\n",
       "  -0.009874829,\n",
       "  0.0059639756,\n",
       "  -0.012575835,\n",
       "  0.023762787,\n",
       "  0.013326953,\n",
       "  0.02896926,\n",
       "  -0.011821084,\n",
       "  -0.01252435,\n",
       "  -0.008376229,\n",
       "  0.019008277,\n",
       "  0.02044472,\n",
       "  0.0152274985,\n",
       "  -0.009830934,\n",
       "  0.0038032155,\n",
       "  0.01705905,\n",
       "  -0.01037302,\n",
       "  -0.019852586,\n",
       "  0.0056042955,\n",
       "  0.002123212,\n",
       "  -0.001277492,\n",
       "  0.0068871505,\n",
       "  0.004105404,\n",
       "  -0.009318579,\n",
       "  0.033923693,\n",
       "  0.0004621371,\n",
       "  0.008851756,\n",
       "  -0.0099100005,\n",
       "  0.0054396917,\n",
       "  0.023847908,\n",
       "  -0.005862014,\n",
       "  0.0076933987,\n",
       "  0.0060127284,\n",
       "  0.043833897,\n",
       "  0.020058535,\n",
       "  -0.003414897,\n",
       "  -0.00031357675,\n",
       "  0.0034935432,\n",
       "  0.0080390815,\n",
       "  -0.0057307435,\n",
       "  -0.012360744,\n",
       "  -0.0016043315,\n",
       "  0.008262082,\n",
       "  0.001309431,\n",
       "  -0.0014000533,\n",
       "  0.021249171,\n",
       "  0.010257607,\n",
       "  0.0051791244,\n",
       "  -0.004678468,\n",
       "  -0.0069245687,\n",
       "  -0.0038779937,\n",
       "  0.030464135,\n",
       "  -0.021595603,\n",
       "  0.012417502,\n",
       "  0.017158775,\n",
       "  0.0064268806,\n",
       "  -0.015434192,\n",
       "  -0.016394645,\n",
       "  -0.0021929897,\n",
       "  0.00063479296,\n",
       "  -0.011412858,\n",
       "  -0.002626083,\n",
       "  0.020014182,\n",
       "  -0.0036072473,\n",
       "  0.0042657778,\n",
       "  0.0017025727,\n",
       "  0.017149882,\n",
       "  0.010127959,\n",
       "  -0.021141425,\n",
       "  -0.0029651793,\n",
       "  -0.0006062833,\n",
       "  -0.0053604366,\n",
       "  -0.018004075,\n",
       "  0.011427091,\n",
       "  0.014249015,\n",
       "  -0.009395248,\n",
       "  0.008112748,\n",
       "  0.01973847,\n",
       "  -0.004837676,\n",
       "  -0.01925171,\n",
       "  -0.017325839,\n",
       "  0.0021451272,\n",
       "  0.013736246,\n",
       "  0.01980646,\n",
       "  -0.00054109923,\n",
       "  -0.0052270223,\n",
       "  0.008452919,\n",
       "  0.008049887,\n",
       "  0.0020180857,\n",
       "  0.025284745,\n",
       "  0.009431715,\n",
       "  -0.0029525629,\n",
       "  -0.0014390595,\n",
       "  0.002651495,\n",
       "  -0.007226143,\n",
       "  0.0020057042,\n",
       "  0.009863765,\n",
       "  -0.014640779,\n",
       "  0.01875615,\n",
       "  0.027974132,\n",
       "  0.03341696,\n",
       "  0.0097338045,\n",
       "  -0.020462269,\n",
       "  0.006183033,\n",
       "  -0.000737044,\n",
       "  0.0010450992,\n",
       "  -0.001797701,\n",
       "  0.0146232825,\n",
       "  0.01557538,\n",
       "  0.017227188,\n",
       "  0.021973103,\n",
       "  -0.0003609131,\n",
       "  -0.0055517275,\n",
       "  -0.020035658,\n",
       "  0.015374157,\n",
       "  0.0031520764,\n",
       "  0.005511659,\n",
       "  -0.010244175,\n",
       "  -0.008479008,\n",
       "  -0.0023032755,\n",
       "  -0.023286078,\n",
       "  -0.023021115,\n",
       "  0.0117404,\n",
       "  0.004048565,\n",
       "  0.0322189,\n",
       "  0.005477093,\n",
       "  -0.01246395,\n",
       "  -0.012305647,\n",
       "  0.0059004473,\n",
       "  -0.0006489307,\n",
       "  -0.01635449,\n",
       "  -0.004546578,\n",
       "  0.0064304247,\n",
       "  0.0075365263,\n",
       "  0.007524201,\n",
       "  0.0076568555,\n",
       "  -0.006197999,\n",
       "  -0.004063057,\n",
       "  -0.0022034708,\n",
       "  0.00033828992,\n",
       "  -0.022496525,\n",
       "  -0.0071912915,\n",
       "  0.010501731,\n",
       "  0.024100069,\n",
       "  -0.020483004,\n",
       "  -0.016684623,\n",
       "  -0.01761629,\n",
       "  0.009209141,\n",
       "  -0.008352923,\n",
       "  0.0069636987,\n",
       "  -0.022359425,\n",
       "  -0.008433241,\n",
       "  0.008250108,\n",
       "  -0.009077387,\n",
       "  0.0016262531,\n",
       "  -0.007839539,\n",
       "  -0.0043501914,\n",
       "  0.0044611553,\n",
       "  0.024938961,\n",
       "  -0.012160886,\n",
       "  0.00053622935,\n",
       "  0.0039932174,\n",
       "  -0.02224805,\n",
       "  0.023106508,\n",
       "  -0.013806752,\n",
       "  0.0022317194,\n",
       "  -0.014494062,\n",
       "  0.006414128,\n",
       "  0.0062246737,\n",
       "  0.011524937,\n",
       "  -0.0004938436,\n",
       "  0.012708179,\n",
       "  -0.013239621,\n",
       "  -0.0016129087,\n",
       "  -0.017683642,\n",
       "  -0.008986169,\n",
       "  0.01207665,\n",
       "  -0.028060745,\n",
       "  -0.0005984009,\n",
       "  -0.004602681,\n",
       "  0.007177218,\n",
       "  0.0106634535,\n",
       "  -0.014768925,\n",
       "  0.009135054,\n",
       "  0.016967159,\n",
       "  -0.011235765,\n",
       "  0.014122761,\n",
       "  -0.00075586315,\n",
       "  -0.011715677,\n",
       "  0.015174444,\n",
       "  0.0041108914,\n",
       "  -0.031942,\n",
       "  0.008056523,\n",
       "  -0.003763195,\n",
       "  -0.004150814,\n",
       "  0.01860292,\n",
       "  0.005298585,\n",
       "  -0.006498082,\n",
       "  -0.010007543,\n",
       "  -0.0017439347,\n",
       "  -0.015782451,\n",
       "  0.0044554584,\n",
       "  0.0047532585,\n",
       "  0.01191638,\n",
       "  0.008291798,\n",
       "  0.006965459,\n",
       "  -0.014760498,\n",
       "  -0.010028789,\n",
       "  -0.019784026,\n",
       "  0.00707709,\n",
       "  -0.0042965803,\n",
       "  -0.014677434,\n",
       "  -0.026670739,\n",
       "  -0.00078472093,\n",
       "  -0.009548668,\n",
       "  0.010104133,\n",
       "  0.014429685,\n",
       "  -0.0022887792,\n",
       "  -0.0048865736,\n",
       "  0.009411912,\n",
       "  0.015031476,\n",
       "  -0.025057923,\n",
       "  0.003952941,\n",
       "  -0.01498156,\n",
       "  -0.0109128095,\n",
       "  0.01313905,\n",
       "  -0.013447736,\n",
       "  0.010339691,\n",
       "  0.00026769337,\n",
       "  -0.0023177546,\n",
       "  -0.011611789,\n",
       "  -0.01040438,\n",
       "  0.011324535,\n",
       "  0.005554042,\n",
       "  -0.0009453498,\n",
       "  0.0062918877,\n",
       "  0.01237901,\n",
       "  0.005625387,\n",
       "  0.0108490465,\n",
       "  -0.0046156263,\n",
       "  0.0029280568,\n",
       "  -0.004089563,\n",
       "  -0.015140617,\n",
       "  -0.00079081365,\n",
       "  0.005531288,\n",
       "  0.0076037673,\n",
       "  -0.0046223067,\n",
       "  -0.015331464,\n",
       "  -0.02883052,\n",
       "  -0.001591052,\n",
       "  0.01777138,\n",
       "  -0.014059563,\n",
       "  -0.012437106,\n",
       "  -0.013897008,\n",
       "  -0.020670926,\n",
       "  -0.010533061,\n",
       "  -0.022214256,\n",
       "  -0.01422142,\n",
       "  -0.022019057,\n",
       "  -0.005626191,\n",
       "  0.020789038,\n",
       "  0.027188813,\n",
       "  -0.007378846,\n",
       "  0.015847247,\n",
       "  -0.020108357,\n",
       "  -0.0056007975,\n",
       "  0.026003847,\n",
       "  -0.007780016,\n",
       "  -0.0045389235,\n",
       "  -0.007171423,\n",
       "  -0.016819442,\n",
       "  -0.008184711,\n",
       "  -0.0057788882,\n",
       "  -0.013807943,\n",
       "  -0.0049088704,\n",
       "  0.0129920095,\n",
       "  0.036725327,\n",
       "  0.020435698,\n",
       "  -0.011265129,\n",
       "  -0.0065259975,\n",
       "  -0.01974225,\n",
       "  -0.030399024,\n",
       "  -0.011904034,\n",
       "  -0.026748791,\n",
       "  -0.011319888,\n",
       "  -0.010018549,\n",
       "  -0.026992198,\n",
       "  0.009084594,\n",
       "  0.011809756,\n",
       "  0.0044721095,\n",
       "  -0.013673457,\n",
       "  -0.01110383,\n",
       "  -0.01398894,\n",
       "  -0.012253725,\n",
       "  0.021440651,\n",
       "  0.0066119223,\n",
       "  -0.010951615,\n",
       "  0.007518258,\n",
       "  0.022812815,\n",
       "  -0.018945368,\n",
       "  0.014332247,\n",
       "  0.001789891,\n",
       "  -0.012310469,\n",
       "  0.0007911368,\n",
       "  0.011678367,\n",
       "  -0.012226061,\n",
       "  -0.0011801503,\n",
       "  0.011583368,\n",
       "  -0.02845204,\n",
       "  -0.024661276,\n",
       "  0.010267821,\n",
       "  -0.020183977,\n",
       "  0.008336419,\n",
       "  -0.014763521,\n",
       "  0.025316816,\n",
       "  -0.015382842,\n",
       "  0.018122872,\n",
       "  -0.008345222,\n",
       "  0.011549787,\n",
       "  0.0080933,\n",
       "  0.007970972,\n",
       "  -0.021784583,\n",
       "  -0.0008949709,\n",
       "  -0.0068962295,\n",
       "  0.01839756,\n",
       "  -0.0025989194,\n",
       "  -0.0010020348,\n",
       "  -0.0103374645,\n",
       "  0.03568843,\n",
       "  -0.01939031,\n",
       "  -0.0072121234,\n",
       "  -0.0147484355,\n",
       "  0.018122504,\n",
       "  -0.013899039,\n",
       "  -0.009741858,\n",
       "  0.015900198,\n",
       "  0.0045708,\n",
       "  -0.007677467,\n",
       "  0.00651831,\n",
       "  0.002375558,\n",
       "  0.00046634531,\n",
       "  -0.0066671395,\n",
       "  -0.027555829,\n",
       "  0.002248922,\n",
       "  0.010708816,\n",
       "  -0.0036742932,\n",
       "  0.016983047,\n",
       "  0.009290781,\n",
       "  0.01644825,\n",
       "  -0.013598623,\n",
       "  -0.03048552,\n",
       "  0.0050101606,\n",
       "  0.01891538,\n",
       "  0.0045062345,\n",
       "  0.02486436,\n",
       "  0.0021229577,\n",
       "  0.014305446,\n",
       "  -0.003853054,\n",
       "  -0.013354966,\n",
       "  0.002152624,\n",
       "  0.004737787,\n",
       "  -0.025273737,\n",
       "  -0.020786552,\n",
       "  -0.0019360767,\n",
       "  0.015463818,\n",
       "  0.004701651,\n",
       "  -0.008099923,\n",
       "  -0.003526991,\n",
       "  -0.01678933,\n",
       "  -0.017506054,\n",
       "  0.020293262,\n",
       "  0.01609046,\n",
       "  0.011319844,\n",
       "  0.02700665,\n",
       "  0.03358505,\n",
       "  -0.002420232,\n",
       "  0.015083354,\n",
       "  -0.004971266,\n",
       "  -0.0077412566,\n",
       "  -0.0034262186,\n",
       "  -0.010357952,\n",
       "  -0.003013506,\n",
       "  -0.021606075,\n",
       "  -0.0069599664,\n",
       "  -0.024525402,\n",
       "  -0.001234848,\n",
       "  0.0062361946,\n",
       "  -0.0005144314,\n",
       "  -0.029847644,\n",
       "  0.0013315195,\n",
       "  -0.02194999,\n",
       "  0.0049464367,\n",
       "  0.016496789,\n",
       "  0.020614978,\n",
       "  0.0219635,\n",
       "  0.0124381455,\n",
       "  -0.029641628,\n",
       "  -0.0012357609,\n",
       "  0.0071890973,\n",
       "  0.008625994,\n",
       "  0.033320714,\n",
       "  0.005074589,\n",
       "  -0.0038560703,\n",
       "  0.0141756125,\n",
       "  0.009449744,\n",
       "  -0.0076319277,\n",
       "  0.0054410873,\n",
       "  -0.0133254025,\n",
       "  -0.00029074843,\n",
       "  0.01953052,\n",
       "  0.0073961425,\n",
       "  0.017694334,\n",
       "  0.001431969,\n",
       "  0.0015695525,\n",
       "  0.002556178,\n",
       "  -0.014454152,\n",
       "  0.010981467,\n",
       "  -0.0071924953,\n",
       "  -0.018249696,\n",
       "  0.01719995,\n",
       "  0.0057538208,\n",
       "  0.01999249,\n",
       "  -0.005923725,\n",
       "  -0.014184902,\n",
       "  -0.014390113,\n",
       "  0.00131512,\n",
       "  -0.0011182462,\n",
       "  0.0020898683,\n",
       "  -0.0070328494,\n",
       "  0.0019610801,\n",
       "  0.0018945768,\n",
       "  0.012214859,\n",
       "  0.0056419224,\n",
       "  0.0034868256,\n",
       "  0.0070319474,\n",
       "  0.0184624,\n",
       "  -0.0017732604,\n",
       "  -0.013028859,\n",
       "  0.0063763023,\n",
       "  0.014209856,\n",
       "  0.005445999,\n",
       "  -0.015229886,\n",
       "  0.017073153,\n",
       "  -0.019984301,\n",
       "  0.007169556,\n",
       "  -0.0074545564,\n",
       "  0.0007839735,\n",
       "  -0.02426278,\n",
       "  -0.015839556,\n",
       "  0.016419925,\n",
       "  -0.0054424806,\n",
       "  -0.0070915786,\n",
       "  -0.021306645,\n",
       "  0.014155113,\n",
       "  0.014292766,\n",
       "  -0.0046233106,\n",
       "  0.019160807,\n",
       "  -0.03280809,\n",
       "  -0.01512427,\n",
       "  0.006731367,\n",
       "  0.002114262,\n",
       "  -0.0073543736,\n",
       "  0.009599517,\n",
       "  0.0060790027,\n",
       "  -0.026490754,\n",
       "  0.016315244,\n",
       "  -0.025039488,\n",
       "  0.0051479954,\n",
       "  -0.00042565225,\n",
       "  -0.012844226,\n",
       "  0.0012486266,\n",
       "  0.008362089,\n",
       "  0.0035061967,\n",
       "  0.00021352341,\n",
       "  -0.008769957,\n",
       "  0.021754147,\n",
       "  0.007543759,\n",
       "  -0.00055194506,\n",
       "  0.0033517277,\n",
       "  -0.01286553,\n",
       "  0.015252265,\n",
       "  4.9735565e-05,\n",
       "  0.011204595,\n",
       "  -0.028870033,\n",
       "  -0.017379167,\n",
       "  -0.0034718947,\n",
       "  -0.0048425994,\n",
       "  0.0016248794,\n",
       "  -0.0017752764,\n",
       "  0.007029915,\n",
       "  0.006015289,\n",
       "  -0.0021359865,\n",
       "  0.012830352,\n",
       "  0.02325288,\n",
       "  0.019339511,\n",
       "  0.0025270688,\n",
       "  -0.0069935825,\n",
       "  -0.014255929,\n",
       "  -0.009124801,\n",
       "  -0.02909756,\n",
       "  -0.0064285225,\n",
       "  0.013380631,\n",
       "  -0.005974016,\n",
       "  0.002422451,\n",
       "  0.003742077,\n",
       "  -0.007649362,\n",
       "  -0.018664239,\n",
       "  0.0014242701,\n",
       "  0.020668507,\n",
       "  -0.014600252,\n",
       "  -0.014588006,\n",
       "  0.005832739,\n",
       "  0.002825178,\n",
       "  -0.008951095,\n",
       "  -0.004699889,\n",
       "  0.0082078045,\n",
       "  0.0105440095,\n",
       "  0.007884411,\n",
       "  0.0074230726,\n",
       "  0.011439777,\n",
       "  0.018586926,\n",
       "  -0.0068064188,\n",
       "  5.0378225e-05,\n",
       "  0.019567695,\n",
       "  -0.00094011426,\n",
       "  0.029918006,\n",
       "  0.014829611,\n",
       "  0.0013182858,\n",
       "  0.02252145,\n",
       "  -0.012278117,\n",
       "  0.016652575,\n",
       "  -0.00034160263,\n",
       "  0.003528075,\n",
       "  -0.009338817,\n",
       "  -0.018995667,\n",
       "  0.0056400374,\n",
       "  0.004261854,\n",
       "  0.005096224,\n",
       "  0.03226831,\n",
       "  0.00085560005,\n",
       "  0.005096831,\n",
       "  -0.009687555,\n",
       "  0.004089385,\n",
       "  -0.001045414,\n",
       "  0.007349471,\n",
       "  0.0004042196,\n",
       "  0.00036870356,\n",
       "  0.022097873,\n",
       "  0.0016453351,\n",
       "  -0.01747002,\n",
       "  0.0035052106,\n",
       "  -0.017877258,\n",
       "  -0.01775403,\n",
       "  0.00948042,\n",
       "  0.00048795395,\n",
       "  0.016544176,\n",
       "  -0.022638185,\n",
       "  0.005950978,\n",
       "  0.021134894,\n",
       "  -0.03095957,\n",
       "  0.035776358,\n",
       "  0.0018161901,\n",
       "  -0.00017982896,\n",
       "  0.0024913514,\n",
       "  -0.00090268394,\n",
       "  -0.012027852,\n",
       "  0.040664352,\n",
       "  -0.0007098581,\n",
       "  0.023048008,\n",
       "  -0.0057810047,\n",
       "  -0.021075834,\n",
       "  0.031756952,\n",
       "  0.0005256945,\n",
       "  -0.012676366,\n",
       "  -0.0019357724,\n",
       "  0.028844045,\n",
       "  -0.003930382,\n",
       "  -0.0066673113,\n",
       "  -0.017049275,\n",
       "  -0.007875136,\n",
       "  -0.0031563495,\n",
       "  -0.0026315372,\n",
       "  -0.0015598849,\n",
       "  -0.008563806,\n",
       "  -0.009175025,\n",
       "  0.012953845,\n",
       "  -0.007613103,\n",
       "  0.008025906,\n",
       "  0.014172173,\n",
       "  0.019425148,\n",
       "  -0.015965888,\n",
       "  -0.0087846285,\n",
       "  -0.018059948,\n",
       "  -0.007381117,\n",
       "  0.013403218,\n",
       "  -0.0011057195,\n",
       "  -0.01624885,\n",
       "  0.017662758,\n",
       "  -0.0039566765,\n",
       "  -0.004560227,\n",
       "  -0.026993755,\n",
       "  0.010438963,\n",
       "  -0.009596013,\n",
       "  -0.02225156,\n",
       "  -0.0040815766,\n",
       "  -0.0008787129,\n",
       "  -0.0088638365,\n",
       "  0.011119326,\n",
       "  -0.012113467,\n",
       "  -0.0013417564,\n",
       "  -0.015642677,\n",
       "  0.0011310901,\n",
       "  -0.008255963,\n",
       "  0.011658444,\n",
       "  0.0017269783,\n",
       "  0.011964085,\n",
       "  -0.030788383,\n",
       "  0.0021514576,\n",
       "  0.006962503,\n",
       "  0.0019304501,\n",
       "  0.0015900421,\n",
       "  -0.0056342757,\n",
       "  0.0020484864,\n",
       "  -0.0063187294,\n",
       "  -0.017287727,\n",
       "  -0.02337892,\n",
       "  0.0050210916,\n",
       "  -0.014883296,\n",
       "  -0.0122193815,\n",
       "  0.013319903,\n",
       "  -0.008152469,\n",
       "  -0.0022460495,\n",
       "  0.0020330672,\n",
       "  0.016870879,\n",
       "  0.020527516,\n",
       "  -0.0073058484,\n",
       "  0.0037367363,\n",
       "  -0.014195252,\n",
       "  0.0100968145,\n",
       "  -0.02520214,\n",
       "  -0.0069164056,\n",
       "  0.009912929,\n",
       "  0.005949336,\n",
       "  0.022629606,\n",
       "  -0.0347882,\n",
       "  0.008244908,\n",
       "  0.006139173,\n",
       "  0.007830606,\n",
       "  -0.01452509,\n",
       "  0.011710856,\n",
       "  0.0034314708,\n",
       "  0.017089503,\n",
       "  0.008533257,\n",
       "  0.01552324,\n",
       "  -0.009250274,\n",
       "  0.018381216,\n",
       "  -0.01893301,\n",
       "  -0.01766363,\n",
       "  -0.0031653857,\n",
       "  0.00031112923,\n",
       "  -0.004213986,\n",
       "  -0.005587437,\n",
       "  -0.006788647,\n",
       "  0.021331757,\n",
       "  0.013328032,\n",
       "  0.00062227895,\n",
       "  0.00557,\n",
       "  0.01693554,\n",
       "  0.00085166364,\n",
       "  0.01418387,\n",
       "  0.0038359456,\n",
       "  0.08137688,\n",
       "  0.004674641,\n",
       "  0.007131261,\n",
       "  0.004304089,\n",
       "  0.031423993,\n",
       "  -0.020499174,\n",
       "  4.5657785e-07,\n",
       "  -0.011345544,\n",
       "  -0.015793627,\n",
       "  0.0007564529,\n",
       "  -0.009200186,\n",
       "  0.0089874165,\n",
       "  -0.02914141,\n",
       "  -0.016556056,\n",
       "  0.015144961,\n",
       "  -0.0040608896,\n",
       "  0.016945163,\n",
       "  0.016526965,\n",
       "  0.013019142,\n",
       "  -0.0077923797,\n",
       "  -0.001959878,\n",
       "  0.000620577,\n",
       "  0.00687914,\n",
       "  0.008253336,\n",
       "  -0.011696248,\n",
       "  0.01816925,\n",
       "  0.0014339399,\n",
       "  0.0061184075,\n",
       "  0.0073750443,\n",
       "  -0.0146322865,\n",
       "  -0.0079085175,\n",
       "  0.009554041,\n",
       "  0.0004305948,\n",
       "  -0.013598303,\n",
       "  0.010465532,\n",
       "  -0.0083946,\n",
       "  0.009247565,\n",
       "  0.017409323,\n",
       "  -0.027900774,\n",
       "  0.021667428,\n",
       "  -3.031183e-05,\n",
       "  0.009648078,\n",
       "  0.0011255033,\n",
       "  -0.0050701858,\n",
       "  0.0048651877,\n",
       "  0.0038898475,\n",
       "  -0.0038360811,\n",
       "  0.0041092173,\n",
       "  0.028249947,\n",
       "  0.005885681,\n",
       "  -0.0008003062,\n",
       "  -0.009059399,\n",
       "  0.012768046,\n",
       "  -0.010200753,\n",
       "  0.02203555,\n",
       "  -0.015671019,\n",
       "  -0.0069447756,\n",
       "  -0.0032066875,\n",
       "  0.010725532,\n",
       "  0.0021446692,\n",
       "  -0.0006518995,\n",
       "  -0.015097485,\n",
       "  0.02224382,\n",
       "  0.017957661,\n",
       "  0.011881866,\n",
       "  -0.013077895,\n",
       "  0.0006986527,\n",
       "  0.005188489,\n",
       "  0.009427195,\n",
       "  -0.00057804363,\n",
       "  -0.007772159,\n",
       "  0.010876514,\n",
       "  0.014954084,\n",
       "  -0.01880181,\n",
       "  -0.012300566,\n",
       "  0.033035304,\n",
       "  -0.0062331064,\n",
       "  -0.020329893,\n",
       "  -0.00902695,\n",
       "  0.02208542,\n",
       "  0.01773994,\n",
       "  0.01928239,\n",
       "  0.016189083,\n",
       "  -0.007253299,\n",
       "  0.0031268687,\n",
       "  -0.0016400727,\n",
       "  -0.021529056,\n",
       "  0.005278882,\n",
       "  0.00096737995,\n",
       "  -0.005005012,\n",
       "  0.02938541,\n",
       "  -0.010237935,\n",
       "  -0.0034028564,\n",
       "  0.011328179,\n",
       "  -0.017398069,\n",
       "  -0.0041802977,\n",
       "  -0.014953774,\n",
       "  -0.0038843977,\n",
       "  -0.018746013,\n",
       "  0.005559999,\n",
       "  0.012394308,\n",
       "  0.0028909042,\n",
       "  -0.0029505158,\n",
       "  -0.009879281,\n",
       "  0.0005447431,\n",
       "  0.014260865,\n",
       "  -0.0036915543,\n",
       "  -0.0018363293,\n",
       "  0.0041595874,\n",
       "  -0.003898952,\n",
       "  -0.0032444382,\n",
       "  -0.017536683,\n",
       "  -0.01433636,\n",
       "  0.0053214557,\n",
       "  0.012520159,\n",
       "  0.009919582,\n",
       "  0.0015208467,\n",
       "  0.012292914,\n",
       "  0.009198117,\n",
       "  -0.015886836,\n",
       "  -0.012340267,\n",
       "  0.0015968306,\n",
       "  0.026717918,\n",
       "  -0.003110564,\n",
       "  -0.019908654,\n",
       "  -0.0040689656,\n",
       "  -0.004652618,\n",
       "  0.0048158155,\n",
       "  0.002670742,\n",
       "  0.002996114,\n",
       "  0.012554782,\n",
       "  -0.0054360353,\n",
       "  -0.011960451,\n",
       "  0.0056740763,\n",
       "  0.00717008,\n",
       "  -0.008981078,\n",
       "  0.0005163833,\n",
       "  -0.0055454536,\n",
       "  -0.017589396,\n",
       "  0.008811825,\n",
       "  -0.0097806435,\n",
       "  -0.002028527,\n",
       "  -0.013982837,\n",
       "  -0.02335421,\n",
       "  0.004873081,\n",
       "  -0.0035505008,\n",
       "  -0.0034067065,\n",
       "  0.0431018,\n",
       "  -0.01181877,\n",
       "  -0.005712876,\n",
       "  -0.001491861,\n",
       "  0.006299543,\n",
       "  -0.028998183,\n",
       "  -0.0133041255,\n",
       "  -0.01697734,\n",
       "  -0.0155174425,\n",
       "  0.007409217,\n",
       "  0.0038414148,\n",
       "  0.014331027,\n",
       "  -0.010728209,\n",
       "  -0.01862473,\n",
       "  -0.008437126,\n",
       "  0.008506316,\n",
       "  0.018121019,\n",
       "  -8.338236e-05,\n",
       "  0.0026962666,\n",
       "  0.030305494,\n",
       "  -0.019940173,\n",
       "  0.03717113,\n",
       "  0.00900792,\n",
       "  -0.011995886,\n",
       "  0.017322846,\n",
       "  0.008637301,\n",
       "  0.016366638,\n",
       "  -0.024804011,\n",
       "  -0.011236625,\n",
       "  -0.004103586,\n",
       "  -0.0035348556,\n",
       "  0.029569449,\n",
       "  -0.010401016,\n",
       "  -0.021543898,\n",
       "  0.013304569,\n",
       "  0.0040611266,\n",
       "  -0.0038322327,\n",
       "  -0.008676146,\n",
       "  0.0018051161,\n",
       "  -0.00016899646,\n",
       "  0.026906235,\n",
       "  -0.010903368,\n",
       "  0.006089249,\n",
       "  0.007622684,\n",
       "  0.015565302,\n",
       "  0.01802951,\n",
       "  0.011561189,\n",
       "  -0.0033245366,\n",
       "  0.018712325,\n",
       "  0.011179734,\n",
       "  0.0025263971,\n",
       "  -0.005457303,\n",
       "  -0.009388876,\n",
       "  -0.007089028,\n",
       "  0.01567987,\n",
       "  0.010161868,\n",
       "  0.0068123206,\n",
       "  -0.002598721,\n",
       "  -0.0075673466,\n",
       "  0.021128995,\n",
       "  -0.012607475,\n",
       "  0.0060277428,\n",
       "  -0.012847605,\n",
       "  0.0046634176,\n",
       "  -0.011996144,\n",
       "  0.017324883,\n",
       "  ...]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "vectors = embeddings.embed_documents(split_docs[0].page_content)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector stores\n",
    "LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n",
    "https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Now that we have our vectors in the vector store we can Perform things like similarity search etc.\n",
    "res = vector_store.similarity_search(\n",
    "    \"What is attention \"\n",
    ")\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1.6813855\n",
      "Document page_content='1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2' metadata={'source': '../attention is all you need.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "res = vector_store.similarity_search_with_score(\"WHat is The goal of reducing sequential computation\")\n",
    "doc,score = res[0]\n",
    "print(\"Score\",score)\n",
    "print(\"Document\",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
